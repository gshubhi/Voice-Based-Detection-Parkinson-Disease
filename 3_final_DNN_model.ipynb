{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a9d60cd-a9a8-4298-be4f-1629a3ff85f1",
   "metadata": {},
   "source": [
    "dnn 2 classifier - results for all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532edf85-bc07-448b-abfd-1532db9bbb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 20.725958585739136 seconds\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2773 - accuracy: 0.9575\n",
      "Final Training Accuracy: 0.9574999809265137\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5207 - accuracy: 0.8825\n",
      "Final Testing Accuracy: 0.8824999928474426\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import time\n",
    "\n",
    "# Custom callback to print training and testing accuracy\n",
    "class AccuracyCallback(Callback):\n",
    "    def __init__(self, x_train, y_train, x_test, y_test):\n",
    "        super().__init__()\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_loss, train_accuracy = self.model.evaluate(self.x_train, self.y_train, verbose=0)\n",
    "        test_loss, test_accuracy = self.model.evaluate(self.x_test, self.y_test, verbose=0)\n",
    "        print(f\"Epoch {epoch + 1}/{self.params['epochs']}, \"\n",
    "              f\"Training Accuracy: {train_accuracy:.4f}, \"\n",
    "              f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Load your CSV file\n",
    "file_path = r\"C:\\MAJOR PROJECT\\NOTEBOOKmajorProject\\featureAnalysis+classification\\group1_single\\spon\\allSpon.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming your CSV file has columns/features and a target column named 'labels'\n",
    "X = df.drop('labels', axis=1)\n",
    "y = df['labels']\n",
    "\n",
    "# Preprocess the data (standardization and label encoding)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "start_time = time.time()\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Dense(128, input_shape=(x_train.shape[1],), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "# Record the current time after training\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time = end_time - start_time\n",
    "print(\"Training time:\", training_time, \"seconds\")\n",
    "# Evaluate the model on training data to get final training accuracy\n",
    "final_train_loss, final_train_accuracy = model.evaluate(x_train, y_train)\n",
    "print(\"Final Training Accuracy:\", final_train_accuracy)\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Final Testing Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0b991-60a8-4e38-aa4b-9d7885d6ce09",
   "metadata": {},
   "source": [
    "## to print training time\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# Record the current time before training\n",
    "start_time = time.time()\n",
    "\n",
    "# Your training code here\n",
    "# Assuming you're training your XGBoost model and have stored it in xgb_model\n",
    "\n",
    "# Record the current time after training\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the time taken for training\n",
    "training_time = end_time - start_time\n",
    "print(\"Training time:\", training_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb3e0b-ccb0-4445-a88b-4675fce50b09",
   "metadata": {},
   "source": [
    "crow search algo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8047c-a1da-408e-9438-2e2f9d2e6d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
