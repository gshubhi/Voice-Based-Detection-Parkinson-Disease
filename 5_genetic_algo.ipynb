{"cells":[{"cell_type":"code","execution_count":null,"id":"f8630858-3873-4a58-bc15-e266d16ab275","metadata":{"id":"f8630858-3873-4a58-bc15-e266d16ab275","outputId":"42da2b92-6944-4f6b-e7b8-e223a946cda1"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_24308\\2578652469.py:12: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n","  chromosome = np.ones(n_feat, dtype=np.bool)\n"]},{"ename":"AttributeError","evalue":"module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 101\u001b[0m\n\u001b[0;32m     98\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, category\u001b[38;5;241m=\u001b[39mConvergenceWarning)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Run genetic algorithm for feature selection\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m best_chromo, best_score \u001b[38;5;241m=\u001b[39m \u001b[43mgenerations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_parents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutation_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Extract selected features\u001b[39;00m\n\u001b[0;32m    105\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mcolumns[np\u001b[38;5;241m.\u001b[39marray(best_chromo[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])]\n","Cell \u001b[1;32mIn[1], line 60\u001b[0m, in \u001b[0;36mgenerations\u001b[1;34m(df, label, size, n_feat, n_parents, mutation_rate, n_gen, X_train, X_test, Y_train, Y_test)\u001b[0m\n\u001b[0;32m     58\u001b[0m best_chromo \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     59\u001b[0m best_score \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 60\u001b[0m population_nextgen \u001b[38;5;241m=\u001b[39m \u001b[43minitilization_of_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_feat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_gen):\n\u001b[0;32m     62\u001b[0m     scores, pop_after_fit \u001b[38;5;241m=\u001b[39m fitness_score(population_nextgen, X_train, X_test, Y_train, Y_test)\n","Cell \u001b[1;32mIn[1], line 12\u001b[0m, in \u001b[0;36minitilization_of_population\u001b[1;34m(size, n_feat)\u001b[0m\n\u001b[0;32m     10\u001b[0m population \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(size):\n\u001b[1;32m---> 12\u001b[0m     chromosome \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(n_feat, dtype\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m)\n\u001b[0;32m     13\u001b[0m     chromosome[:\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.3\u001b[39m \u001b[38;5;241m*\u001b[39m n_feat)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(chromosome)\n","File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\__init__.py:338\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    333\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtesting\u001b[39;00m\n","\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"]}],"source":["#genetic algo fs\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import cross_val_score\n","\n","def initilization_of_population(size, n_feat):\n","    population = []\n","    for i in range(size):\n","        chromosome = np.ones(n_feat, dtype=np.bool)\n","        chromosome[:int(0.3 * n_feat)] = False\n","        np.random.shuffle(chromosome)\n","        population.append(chromosome)\n","    return population\n","\n","def fitness_score(population, X_train, X_test, Y_train, Y_test):\n","    scores = []\n","    for chromosome in population:\n","        logmodel.fit(X_train.iloc[:, chromosome], Y_train)\n","        predictions = logmodel.predict(X_test.iloc[:, chromosome])\n","        scores.append(accuracy_score(Y_test, predictions))\n","    scores, population = np.array(scores), np.array(population)\n","    inds = np.argsort(scores)\n","    return list(scores[inds][::-1]), list(population[inds, :][::-1])\n","\n","def selection(pop_after_fit, n_parents):\n","    population_nextgen = []\n","    for i in range(n_parents):\n","        population_nextgen.append(pop_after_fit[i])\n","    return population_nextgen\n","\n","def crossover(pop_after_sel):\n","    pop_nextgen = pop_after_sel\n","    for i in range(0, len(pop_after_sel), 2):\n","        new_par = []\n","        child_1, child_2 = pop_nextgen[i], pop_nextgen[i + 1]\n","        new_par = np.concatenate((child_1[:len(child_1) // 2], child_2[len(child_1) // 2:]))\n","        pop_nextgen.append(new_par)\n","    return pop_nextgen\n","\n","def mutation(pop_after_cross, mutation_rate, n_feat):\n","    mutation_range = int(mutation_rate * n_feat)\n","    pop_next_gen = []\n","    for n in range(0, len(pop_after_cross)):\n","        chromo = pop_after_cross[n]\n","        rand_posi = []\n","        for i in range(0, mutation_range):\n","            pos = np.random.randint(0, n_feat - 1)\n","            rand_posi.append(pos)\n","        for j in rand_posi:\n","            chromo[j] = not chromo[j]\n","        pop_next_gen.append(chromo)\n","    return pop_next_gen\n","\n","def generations(df, label, size, n_feat, n_parents, mutation_rate, n_gen, X_train, X_test, Y_train, Y_test):\n","    best_chromo = []\n","    best_score = []\n","    population_nextgen = initilization_of_population(size, n_feat)\n","    for i in range(n_gen):\n","        scores, pop_after_fit = fitness_score(population_nextgen, X_train, X_test, Y_train, Y_test)\n","        print('Best score in generation', i + 1, ':', scores[:1])\n","        pop_after_sel = selection(pop_after_fit, n_parents)\n","        pop_after_cross = crossover(pop_after_sel)\n","        population_nextgen = mutation(pop_after_cross, mutation_rate, n_feat)\n","        best_chromo.append(pop_after_fit[0])\n","        best_score.append(scores[0])\n","    return best_chromo, best_score\n","\n","# Load your CSV file\n","file_path = r\"C:\\MAJOR PROJECT\\NOTEBOOKmajorProject\\featureAnalysis+classification\\group1_single\\read\\all_read.csv\"\n","df = pd.read_csv(file_path)\n","\n","# Assuming your CSV file has features and a target column named 'labels'\n","X = df.drop('labels', axis=1)\n","y = df['labels']\n","\n","# Train-test split\n","X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize logistic regression model\n","logmodel = LogisticRegression(max_iter=1000)\n","\n","# Define parameters for genetic algorithm\n","size = 10  # Population size\n","n_feat = len(X_train.columns)  # Number of features\n","n_parents = 5  # Number of parents to select\n","mutation_rate = 0.1  # Mutation rate\n","n_gen = 5  # Number of generations\n","import warnings\n","from sklearn.exceptions import ConvergenceWarning\n","\n","# Suppress DeprecationWarning\n","warnings.simplefilter(action='ignore', category=DeprecationWarning)\n","\n","# Suppress ConvergenceWarning\n","warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n","\n","# Run genetic algorithm for feature selection\n","best_chromo, best_score = generations(df, 'labels', size, n_feat, n_parents, mutation_rate, n_gen, X_train, X_test, Y_train, Y_test)\n","\n","\n","# Extract selected features\n","selected_features = X_train.columns[np.array(best_chromo[-1])]\n","\n","# Train logistic regression model with selected features\n","logmodel.fit(X_train[selected_features], Y_train)\n","\n","# Evaluate model on test set\n","predictions = logmodel.predict(X_test[selected_features])\n","accuracy = accuracy_score(Y_test, predictions)\n","print(\"Accuracy on test set:\", accuracy)"]},{"cell_type":"code","execution_count":null,"id":"0c6771f1-8956-47e4-bce5-ff20f1ce2579","metadata":{"id":"0c6771f1-8956-47e4-bce5-ff20f1ce2579","outputId":"b9a0ad4a-44ef-42cd-fed6-1c80e6278163"},"outputs":[{"name":"stdout","output_type":"stream","text":["Best score in generation 1 : [0.7669654289372599]\n","Best score in generation 2 : [0.7592829705505761]\n","Best score in generation 3 : [0.7772087067861716]\n","Best score in generation 4 : [0.7567221510883483]\n","Best score in generation 5 : [0.7772087067861716]\n","98/98 [==============================] - 0s 2ms/step\n","25/25 [==============================] - 0s 2ms/step\n","Training Accuracy with Selected Features: 0.7503201024327785\n","Testing Accuracy with Selected Features: 0.7516005121638925\n","Index(['Tonal_Centroid_3', 'Tonal_Centroid_5', 'Tonal_Centroid_6', 'Chroma_2',\n","       'Chroma_5', 'Chroma_6', 'Chroma_9', 'Chroma_11', 'GFCC_1', 'GFCC_3',\n","       ...\n","       'STFT_2', 'STFT_4', 'STFT_5', 'STFT_7', 'STFT_8', 'STFT_9', 'STFT_11',\n","       'Spectral_Centroid_1', 'RMS_1', 'Zero_Crossing_Rate_1'],\n","      dtype='object', length=116)\n"]}],"source":["# Import necessary libraries\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import regularizers\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Genetic Algorithm for Feature Selection\n","def initialization_of_population(size, n_feat):\n","    population = []\n","    for i in range(size):\n","        chromosome = np.ones(n_feat, dtype=bool)\n","        chromosome[:int(0.3 * n_feat)] = False\n","        np.random.shuffle(chromosome)\n","        population.append(chromosome)\n","    return population\n","\n","def fitness_score(population, X_train, X_test, Y_train, Y_test):\n","    scores = []\n","    for chromosome in population:\n","        logmodel.fit(X_train.iloc[:, chromosome], Y_train)\n","        predictions = logmodel.predict(X_test.iloc[:, chromosome])\n","        scores.append(accuracy_score(Y_test, predictions))\n","    scores, population = np.array(scores), np.array(population)\n","    inds = np.argsort(scores)\n","    return list(scores[inds][::-1]), list(population[inds, :][::-1])\n","\n","def selection(pop_after_fit, n_parents):\n","    population_nextgen = []\n","    for i in range(n_parents):\n","        population_nextgen.append(pop_after_fit[i])\n","    return population_nextgen\n","\n","def crossover(pop_after_sel):\n","    pop_nextgen = pop_after_sel\n","    for i in range(0, len(pop_after_sel), 2):\n","        new_par = []\n","        child_1, child_2 = pop_nextgen[i], pop_nextgen[i + 1]\n","        new_par = np.concatenate((child_1[:len(child_1) // 2], child_2[len(child_1) // 2:]))\n","        pop_nextgen.append(new_par)\n","    return pop_nextgen\n","\n","def mutation(pop_after_cross, mutation_rate, n_feat):\n","    mutation_range = int(mutation_rate * n_feat)\n","    pop_next_gen = []\n","    for n in range(0, len(pop_after_cross)):\n","        chromo = pop_after_cross[n]\n","        rand_posi = []\n","        for i in range(0, mutation_range):\n","            pos = np.random.randint(0, n_feat - 1)\n","            rand_posi.append(pos)\n","        for j in rand_posi:\n","            chromo[j] = not chromo[j]\n","        pop_next_gen.append(chromo)\n","    return pop_next_gen\n","\n","def generations(df, label, size, n_feat, n_parents, mutation_rate, n_gen, X_train, X_test, Y_train, Y_test):\n","    best_chromo = []\n","    best_score = []\n","    population_nextgen = initialization_of_population(size, n_feat)\n","    for i in range(n_gen):\n","        scores, pop_after_fit = fitness_score(population_nextgen, X_train, X_test, Y_train, Y_test)\n","        print('Best score in generation', i + 1, ':', scores[:1])\n","        pop_after_sel = selection(pop_after_fit, n_parents)\n","        pop_after_cross = crossover(pop_after_sel)\n","        population_nextgen = mutation(pop_after_cross, mutation_rate, n_feat)\n","        best_chromo.append(pop_after_fit[0])\n","        best_score.append(scores[0])\n","    return best_chromo, best_score\n","\n","# Load Parkinson's CSV data\n","parkinson_data = pd.read_csv(r\"C:\\MAJOR PROJECT\\NOTEBOOKmajorProject\\featureAnalysis+classification\\read+spon_final.csv\")\n","\n","# Separate features and labels\n","X = parkinson_data.drop('labels', axis=1)\n","y = parkinson_data['labels']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Replace label 4 with label 1\n","y_train[y_train == 4] = 1\n","y_test[y_test == 4] = 1\n","\n","\n","# Define parameters for genetic algorithm\n","size = 10  # Population size\n","n_feat = len(X_train.columns)  # Number of features\n","n_parents = 5  # Number of parents to select\n","mutation_rate = 0.1  # Mutation rate\n","n_gen = 5  # Number of generations\n","\n","# Run genetic algorithm for feature selection\n","best_chromo, best_score = generations(parkinson_data, 'labels', size, n_feat, n_parents, mutation_rate, n_gen, X_train, X_test, y_train, y_test)\n","\n","# Extract selected features\n","selected_features = X_train.columns[np.array(best_chromo[-1])]\n","\n","# Initialize and compile DNN model\n","model = Sequential([\n","    Dense(128, input_shape=(len(selected_features),), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n","    BatchNormalization(),\n","    Dropout(0.5),\n","    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n","    BatchNormalization(),\n","    Dropout(0.5),\n","    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n","    BatchNormalization(),\n","    Dropout(0.5),\n","    Dense(len(np.unique(y)), activation='softmax')\n","])\n","\n","optimizer = Adam(learning_rate=0.001)\n","model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model with selected features\n","model.fit(X_train[selected_features], y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n","\n","# Evaluate model on training set\n","train_predictions = np.argmax(model.predict(X_train[selected_features]), axis=-1)\n","train_accuracy = accuracy_score(y_train, train_predictions)\n","\n","# Evaluate model on testing set\n","test_predictions = np.argmax(model.predict(X_test[selected_features]), axis=-1)\n","test_accuracy = accuracy_score(y_test, test_predictions)\n","\n","print(\"Training Accuracy with Selected Features:\", train_accuracy)\n","print(\"Testing Accuracy with Selected Features:\", test_accuracy)\n","print(selected_features)\n"]},{"cell_type":"code","execution_count":null,"id":"6a43f7d4-4bf8-470e-a05e-23abcadc7021","metadata":{"id":"6a43f7d4-4bf8-470e-a05e-23abcadc7021","outputId":"f847eb48-185c-42ed-b9a2-609bd8ff8f1f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Best score in generation 1 : [0.7464788732394366]\n","Best score in generation 2 : [0.7477592829705506]\n","Best score in generation 3 : [0.764404609475032]\n","Best score in generation 4 : [0.7759282970550576]\n","Best score in generation 5 : [0.7746478873239436]\n","98/98 [==============================] - 0s 2ms/step\n","25/25 [==============================] - 0s 3ms/step\n","Training Accuracy with Selected Features: 0.765044814340589\n","Testing Accuracy with Selected Features: 0.7682458386683739\n","Index(['Tonal_Centroid_1', 'Tonal_Centroid_4', 'Tonal_Centroid_5', 'Chroma_2',\n","       'Chroma_3', 'Chroma_6', 'Chroma_8', 'Chroma_10', 'Chroma_11', 'GFCC_3',\n","       'GFCC_6', 'GFCC_8', 'GFCC_13', 'Mel_1', 'Mel_5', 'Mel_7', 'Mel_11',\n","       'Mel_12', 'Mel_21', 'Mel_22', 'Mel_23', 'Mel_24', 'Mel_25', 'Mel_27',\n","       'Mel_28', 'Mel_29', 'Mel_32', 'Mel_36', 'Mel_38', 'Mel_41', 'Mel_43',\n","       'Mel_44', 'Mel_50', 'Mel_52', 'Mel_54', 'Mel_56', 'Mel_57', 'Mel_58',\n","       'Mel_60', 'Mel_69', 'Mel_70', 'Mel_71', 'Mel_72', 'Mel_74', 'Mel_75',\n","       'Mel_76', 'Mel_78', 'Mel_81', 'Mel_85', 'Mel_86', 'Mel_91', 'Mel_92',\n","       'Mel_96', 'Mel_97', 'Mel_98', 'Mel_100', 'Mel_101', 'Mel_103',\n","       'Mel_104', 'Mel_106', 'Mel_107', 'Mel_111', 'Mel_112', 'Mel_114',\n","       'Mel_115', 'Mel_118', 'Mel_122', 'Mel_123', 'Mel_125', 'MFCC_4',\n","       'MFCC_6', 'MFCC_10', 'MFCC_13', 'Spectral_Contrast_2',\n","       'Spectral_Contrast_4', 'Spectral_Contrast_7', 'STFT_2', 'STFT_4',\n","       'STFT_6', 'STFT_7', 'STFT_8', 'STFT_9', 'STFT_11', 'RMS_1'],\n","      dtype='object')\n"]}],"source":["# Modify initialization_of_population function to randomly select 50 features\n","def initialization_of_population(size, n_feat):\n","    population = []\n","    for i in range(size):\n","        chromosome = np.zeros(n_feat, dtype=bool)\n","        selected_features = np.random.choice(n_feat, 50, replace=False)\n","        chromosome[selected_features] = True\n","        population.append(chromosome)\n","    return population\n","\n","# Modify generations function to optimize parameters and implement elitism\n","def generations(df, label, size, n_feat, n_parents, mutation_rate, n_gen, X_train, X_test, Y_train, Y_test):\n","    best_chromo = []\n","    best_score = []\n","    population_nextgen = initialization_of_population(size, n_feat)\n","    for i in range(n_gen):\n","        scores, pop_after_fit = fitness_score(population_nextgen, X_train, X_test, Y_train, Y_test)\n","        print('Best score in generation', i + 1, ':', scores[:1])\n","        pop_after_sel = selection(pop_after_fit, n_parents)\n","        pop_after_cross = crossover(pop_after_sel)\n","        population_nextgen = mutation(pop_after_cross, mutation_rate, n_feat)\n","        best_chromo.append(pop_after_fit[0])\n","        best_score.append(scores[0])\n","    return best_chromo, best_score\n","\n","# Load Parkinson's CSV data\n","parkinson_data = pd.read_csv(r\"C:\\MAJOR PROJECT\\NOTEBOOKmajorProject\\featureAnalysis+classification\\read+spon_final.csv\")\n","start_time=time.time()\n","# Separate features and labels\n","X = parkinson_data.drop('labels', axis=1)\n","y = parkinson_data['labels']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Replace label 4 with label 1\n","y_train[y_train == 4] = 1\n","y_test[y_test == 4] = 1\n","\n","# Define parameters for genetic algorithm\n","size = 10  # Population size\n","n_feat = len(X_train.columns)  # Number of features\n","n_parents = 5  # Number of parents to select\n","mutation_rate = 0.1  # Mutation rate\n","n_gen = 5  # Number of generations\n","\n","# Run genetic algorithm for feature selection\n","best_chromo, best_score = generations(parkinson_data, 'labels', size, n_feat, n_parents, mutation_rate, n_gen, X_train, X_test, y_train, y_test)\n","\n","# Extract selected features\n","selected_features = X_train.columns[np.array(best_chromo[-1])]\n","\n","# Initialize and compile DNN model\n","model = Sequential([\n","    Dense(128, input_shape=(len(selected_features),), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n","    BatchNormalization(),\n","    Dropout(0.5),\n","    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n","    BatchNormalization(),\n","    Dropout(0.5),\n","    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n","    BatchNormalization(),\n","    Dropout(0.5),\n","    Dense(len(np.unique(y)), activation='softmax')\n","])\n","\n","optimizer = Adam(learning_rate=0.001)\n","model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model with selected features\n","model.fit(X_train[selected_features], y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n","\n","# Evaluate model on training set\n","train_predictions = np.argmax(model.predict(X_train[selected_features]), axis=-1)\n","train_accuracy = accuracy_score(y_train, train_predictions)\n","\n","# Evaluate model on testing set\n","test_predictions = np.argmax(model.predict(X_test[selected_features]), axis=-1)\n","test_accuracy = accuracy_score(y_test, test_predictions)\n","training_time = time.time() - start_time\n","print(\"Training time:\", training_time, \"seconds\")\n","print(\"Training Accuracy with Selected Features:\", train_accuracy)\n","print(\"Testing Accuracy with Selected Features:\", test_accuracy)\n","print(selected_features)\n"]},{"cell_type":"code","execution_count":null,"id":"143440e7-1b3b-4eb3-a995-dfd2d154b7d3","metadata":{"scrolled":true,"id":"143440e7-1b3b-4eb3-a995-dfd2d154b7d3","outputId":"59c0ca85-2293-4450-b586-c68143b7456f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Best score in generation 1 : [0.7637795275590551]\n","Best score in generation 2 : [0.7690288713910761]\n","Best score in generation 3 : [0.7690288713910761]\n","Best score in generation 4 : [0.7769028871391076]\n","Best score in generation 5 : [0.7900262467191601]\n","48/48 [==============================] - 0s 2ms/step\n","12/12 [==============================] - 0s 2ms/step\n","Training time: 57.696723222732544 seconds\n","Training Accuracy with Selected Features: 0.7493438320209974\n","Testing Accuracy with Selected Features: 0.7322834645669292\n","Index(['Tonal_Centroid_1', 'Tonal_Centroid_6', 'Chroma_1', 'Chroma_4',\n","       'Chroma_5', 'Chroma_7', 'Chroma_8', 'Chroma_9', 'Chroma_11', 'GFCC_3',\n","       'GFCC_4', 'GFCC_5', 'GFCC_8', 'GFCC_9', 'GFCC_10', 'Mel_2', 'Mel_3',\n","       'Mel_4', 'Mel_6', 'Mel_8', 'Mel_9', 'Mel_12', 'Mel_21', 'Mel_24',\n","       'Mel_26', 'Mel_27', 'Mel_30', 'Mel_31', 'Mel_32', 'Mel_33', 'Mel_35',\n","       'Mel_36', 'Mel_39', 'Mel_44', 'Mel_46', 'Mel_49', 'Mel_51', 'Mel_61',\n","       'Mel_66', 'Mel_67', 'Mel_70', 'Mel_71', 'Mel_74', 'Mel_75', 'Mel_82',\n","       'Mel_83', 'Mel_87', 'Mel_89', 'Mel_98', 'Mel_100', 'Mel_103', 'Mel_107',\n","       'Mel_110', 'Mel_112', 'Mel_113', 'Mel_114', 'Mel_115', 'Mel_118',\n","       'Mel_119', 'Mel_122', 'Mel_123', 'Mel_124', 'Mel_125', 'Mel_126',\n","       'Mel_127', 'MFCC_1', 'MFCC_2', 'MFCC_4', 'MFCC_7', 'MFCC_11', 'MFCC_13',\n","       'Spectral_Contrast_2', 'Spectral_Contrast_4', 'STFT_2', 'STFT_3',\n","       'STFT_4', 'STFT_5', 'STFT_6', 'STFT_8', 'STFT_9', 'Spectral_Centroid_1',\n","       'Zero_Crossing_Rate_1'],\n","      dtype='object')\n","Number of selected features: 82\n","Selected features: Index(['Tonal_Centroid_1', 'Tonal_Centroid_6', 'Chroma_1', 'Chroma_4',\n","       'Chroma_5', 'Chroma_7', 'Chroma_8', 'Chroma_9', 'Chroma_11', 'GFCC_3',\n","       'GFCC_4', 'GFCC_5', 'GFCC_8', 'GFCC_9', 'GFCC_10', 'Mel_2', 'Mel_3',\n","       'Mel_4', 'Mel_6', 'Mel_8', 'Mel_9', 'Mel_12', 'Mel_21', 'Mel_24',\n","       'Mel_26', 'Mel_27', 'Mel_30', 'Mel_31', 'Mel_32', 'Mel_33', 'Mel_35',\n","       'Mel_36', 'Mel_39', 'Mel_44', 'Mel_46', 'Mel_49', 'Mel_51', 'Mel_61',\n","       'Mel_66', 'Mel_67', 'Mel_70', 'Mel_71', 'Mel_74', 'Mel_75', 'Mel_82',\n","       'Mel_83', 'Mel_87', 'Mel_89', 'Mel_98', 'Mel_100', 'Mel_103', 'Mel_107',\n","       'Mel_110', 'Mel_112', 'Mel_113', 'Mel_114', 'Mel_115', 'Mel_118',\n","       'Mel_119', 'Mel_122', 'Mel_123', 'Mel_124', 'Mel_125', 'Mel_126',\n","       'Mel_127', 'MFCC_1', 'MFCC_2', 'MFCC_4', 'MFCC_7', 'MFCC_11', 'MFCC_13',\n","       'Spectral_Contrast_2', 'Spectral_Contrast_4', 'STFT_2', 'STFT_3',\n","       'STFT_4', 'STFT_5', 'STFT_6', 'STFT_8', 'STFT_9', 'Spectral_Centroid_1',\n","       'Zero_Crossing_Rate_1'],\n","      dtype='object')\n"]}],"source":["# Modify initialization_of_population function to randomly select 50 features\n","import time\n","def initialization_of_population(size, n_feat):\n","    population = []\n","    for i in range(size):\n","        chromosome = np.zeros(n_feat, dtype=bool)\n","        selected_features = np.random.choice(n_feat, 50, replace=False)\n","        chromosome[selected_features] = True\n","        population.append(chromosome)\n","    return population\n","\n","# Modify generations function to optimize parameters and implement elitism\n","def generations(df, label, size, n_feat, n_parents, mutation_rate, n_gen, X_train, X_test, Y_train, Y_test):\n","    best_chromo = []\n","    best_score = []\n","    population_nextgen = initialization_of_population(size, n_feat)\n","    for i in range(n_gen):\n","        scores, pop_after_fit = fitness_score(population_nextgen, X_train, X_test, Y_train, Y_test)\n","        print('Best score in generation', i + 1, ':', scores[:1])\n","        pop_after_sel = selection(pop_after_fit, n_parents)\n","        pop_after_cross = crossover(pop_after_sel)\n","        population_nextgen = mutation(pop_after_cross, mutation_rate, n_feat)\n","        best_chromo.append(pop_after_fit[0])\n","        best_score.append(scores[0])\n","    return best_chromo, best_score\n","\n","# Load Parkinson's CSV data\n","parkinson_data = pd.read_csv(r\"C:\\MAJOR PROJECT\\NOTEBOOKmajorProject\\featureAnalysis+classification\\group1_single\\read\\all_read.csv\")\n","start_time=time.time()\n","# Separate features and labels\n","X = parkinson_data.drop('labels', axis=1)\n","y = parkinson_data['labels']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Replace label 4 with label 1\n","y_train[y_train == 4] = 1\n","y_test[y_test == 4] = 1\n","\n","# Define parameters for genetic algorithm\n","size = 10  # Population size\n","n_feat = len(X_train.columns)  # Number of features\n","n_parents = 5  # Number of parents to select\n","mutation_rate = 0.1  # Mutation rate\n","n_gen = 5  # Number of generations\n","\n","# Run genetic algorithm for feature selection\n","best_chromo, best_score = generations(parkinson_data, 'labels', size, n_feat, n_parents, mutation_rate, n_gen, X_train, X_test, y_train, y_test)\n","\n","# Extract selected features\n","selected_features = X_train.columns[np.array(best_chromo[-1])]\n","\n","# Initialize and compile DNN model\n","model = Sequential([\n","    Dense(128, input_shape=(len(selected_features),), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n","    BatchNormalization(),\n","    Dropout(0.5),\n","    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n","    BatchNormalization(),\n","    Dropout(0.5),\n","    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n","    BatchNormalization(),\n","    Dropout(0.5),\n","    Dense(len(np.unique(y)), activation='softmax')\n","])\n","\n","optimizer = Adam(learning_rate=0.001)\n","model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model with selected features\n","model.fit(X_train[selected_features], y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n","\n","# Evaluate model on training set\n","train_predictions = np.argmax(model.predict(X_train[selected_features]), axis=-1)\n","train_accuracy = accuracy_score(y_train, train_predictions)\n","\n","# Evaluate model on testing set\n","test_predictions = np.argmax(model.predict(X_test[selected_features]), axis=-1)\n","test_accuracy = accuracy_score(y_test, test_predictions)\n","training_time = time.time() - start_time\n","print(\"Training time:\", training_time, \"seconds\")\n","print(\"Training Accuracy with Selected Features:\", train_accuracy)\n","print(\"Testing Accuracy with Selected Features:\", test_accuracy)\n","print(selected_features)\n","selected_features_indices = np.array(best_chromo[-1])\n","selected_features = X_train.columns[selected_features_indices]\n","selected_features_count = np.sum(selected_features_indices)\n","\n","print(\"Number of selected features:\", selected_features_count)\n","print(\"Selected features:\", selected_features)"]},{"cell_type":"code","execution_count":null,"id":"24b90a8f-2d0e-4016-8f7b-18f9ac530cbc","metadata":{"id":"24b90a8f-2d0e-4016-8f7b-18f9ac530cbc"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}